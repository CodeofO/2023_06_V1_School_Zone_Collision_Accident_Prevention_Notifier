import os
import random
import pandas as pd
import numpy as np

import cv2
from ultralytics import YOLO
from tracker_testing import Tracker


# NOTE : detect_type ë³„ ì„±ëŠ¥í‰ê°€
# weight(2.5) : ìƒ, 
    # ì›€ì§ì´ëŠ” ê°ì²´ë§Œ ì¸ì‹
    # ê¹œë¹¡ê±°ë¦¼ ì¡´ì¬
# square(3) : ì¤‘ ud*1.5
    # ê°„í˜¹ ë‹¤ë¥¸ ê°ì²´ë„ ì¸ì‹
    # ê¹œë¹¡ê±°ë¦¼ ì¡´ì¬
# exp : ìƒ, e^n
    # ê°„í˜¹ ë‹¤ë¥¸ ê°ì²´ë„ ì¸ì‹
    # ê¹œë¹¡ê±°ë¦¼ ì ìŒ
# exp2 : ìµœìƒ, 2^n
    # ê°„í˜¹ ë‹¤ë¥¸ ê°ì²´ë„ ì¸ì‹
    # ê¹œë¹¡ê±°ë¦¼ ì ìŒ



data_path = '~/OT2/object_tracking_yolov8_deep_sort'

video1 = 'blur_person'
video2 = 'blur_car'
detection_threshold = 0.5
detect_type = 'exp2' # detect_type : weight, square, exp, exp2, expm1
#weight = 1.5 # weight : 2.5 / square : 3 / exp, exp2 : 1~2
weight = 4
ud_square = 1
warning_count = 10
alraming_time = 150 # 300 ~ 500

# etc
colors = [(random.randint(0, 255), random.randint(0, 255), random.randint(0, 255)) for j in range(10)]
cl_name = pd.read_csv(os.path.join(data_path, 'coco_classes.txt'))
class_names = cl_name['class_name'].tolist()


def load_video(data_path, vidoe_name):
    # video_path
    video_path = os.path.join(data_path, 'data', f"{vidoe_name}.mp4")
    
    return cv2.VideoCapture(video_path)


def yolov8(result, detection_threshold, class_names, target_classes):
            
    detections = []

    for r in result.boxes.data.tolist():
        x1, y1, x2, y2, score, class_id = r
        
        x1 = int(x1)
        x2 = int(x2)
        y1 = int(y1)
        y2 = int(y2)
        class_id = int(class_id)
        
        class_name = class_names[class_id] # ğŸ­
        
        #if (score > detection_threshold) & cond_class :
        if (score > detection_threshold) & (class_name in target_classes) : # ğŸ£
            detections.append([x1, y1, x2, y2, score, class_id]) # â›³ï¸


    return detections


def first_track(tracker, sequence, class_names, before_track_infos1, before_track_infos2, target_classes): # tracker : tracker1 or tracker2 / sequence : 1 or 2
    for class_name in target_classes: # ğŸŒ        
        globals()[f'ud_list_{class_name}'] = [] # ğŸŒ
    

    for track in tracker.tracks:
        x1, y1, x2, y2 = track.bbox
        track_id = track.track_id
        #track_ids.append(track_id) # ğŸ­
        class_id = track.class_id # â›³ï¸
        class_name = class_names[class_id]

        # Center point
        bbox_center = ((x2 - x1),(y2 - y1))
        
        if sequence == 1:
            before_track_infos = before_track_infos1
        elif sequence == 2:
            before_track_infos = before_track_infos2

        # Compare to Before
        if len(before_track_infos) > 0:

            for info in before_track_infos:

                #if track_id == info[1]:
                if (track_id == info[1]) & (class_name in target_classes):
                        bx, by = info[0]
                        x, y = bbox_center
                        
                        # Computing Uclidian Distance Between Before and Now
                        ud = np.sqrt((bx - x)**2 + (by - y)**2)
                        ud = ud ** ud_square
                        globals()[f'ud_list_{class_name}'].append(ud) # ğŸ£
    
    # define mean of Uclidian Distances
    ud_mean_dict = dict() # ğŸŒ
    try:
        for class_name in target_classes:
            ud_mean_dict[class_name] = np.mean(globals()[f'ud_list_{class_name}']) # ğŸŒ
    except:
        pass
    
    return ud_mean_dict


def second_track(frame, tracker, sequence, detect_type, ud_mean_dict, warning_text, before_track_infos1, before_track_infos2, warning_count_down, weight=weight, ud_square=ud_square):
            
    # make list for recording frame's tracking infos
    tracks_info = []
    track_ids = [] # ğŸŒ

    # for track 2ï¸âƒ£
    for track in tracker.tracks:
        bbox = track.bbox
        x1, y1, x2, y2 = bbox
        track_id = track.track_id
        track_ids.append(track_id) # ğŸŒ
        class_id = track.class_id # â›³ï¸
        class_name = class_names[class_id]  # í´ë˜ìŠ¤ IDì— í•´ë‹¹í•˜ëŠ” í´ë˜ìŠ¤ ì´ë¦„ ê°€ì ¸ì˜¤ê¸°                

        for key, value in ud_mean_dict.items():
            if key == class_name:
                ud_list_mean = value
        
        # center 
        bbox_center = ((x2 - x1),(y2 - y1))
        
        # record tracks_info
        info = [bbox_center, track_id, class_id]
        tracks_info.append(info)

        if sequence == 1:
            before_track_infos = before_track_infos1
        elif sequence == 2:
            before_track_infos = before_track_infos2

        for info in before_track_infos:
            if track_id == info[1]:                        

                # Computing Uclidian Distance
                bx, by = info[0]
                x, y = bbox_center
                ud = np.sqrt((bx - x) ** 2 + (by - y) ** 2)
                ud = ud ** ud_square


                #Key CODE
                    # í•˜ì´í¼ íŒŒë¼ë¯¸í„° 
                        # ì‚¬ìš©
                            # 1) ê¸°ìš¸ê¸° w
                        # ì‚¬ìš© í•˜ì§€ ì•ŠìŒ
                            # 1) ì„¸ì œê³±
                            # 2) tanh
                            # 3) ìì—°í•¨ìˆ˜ e
                
                # ì°¨ì— ë¹„í•´ì„œ ì‚¬ëŒì€ Identifying Moving Objectì´ ì˜ ì•ˆë¨. ë”°ë¼ì„œ ê°€ì¤‘ì¹˜ ë¶€ì—¬
                if class_name == 'person':
                    ud = np.exp2(ud + (weight * 2)) # â­•ï¸ # ì‚¬ëŒê³¼ ì°¨ê°€ ê²¹ì³¤ì„ ë•Œ ì‚¬ëŒì—ê²Œ ë” ê°•í•œ ê°€ì¤‘ì¹˜ë¥¼ ì£¼ê¸° ìœ„í•¨

                if detect_type == 'weight':
                    cond_ud = (ud > ud_list_mean * weight)
                elif detect_type == 'square':
                    cond_ud = (ud ** weight > ud_list_mean ** weight)
                elif detect_type == 'exp':
                    cond_ud = (np.exp(ud) > np.exp(ud_list_mean) * weight)
                elif detect_type == 'exp2':
                    cond_ud = (np.exp2(ud) > np.exp2(ud_list_mean) * weight) # ğŸ£
                    
                elif detect_type == 'expm1':
                    cond_ud = (np.expm1(ud) > np.expm1(ud_list_mean) * weight)
                
                # Identifying Moving Object
                if cond_ud: # ğŸ­

                    cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 0, 255), 3)
                    cv2.putText(frame, f"MOVING_{track_id}_{class_name}",                                         
                                (int(x1), int(y1) - 10), 
                                cv2.FONT_HERSHEY_SIMPLEX, 0.9, 
                                #colors[track_id % len(colors)], 
                                (0, 0, 255), 
                                thickness=2)
                    
                    print(f'A moving {class_name} detected!!!')
                    
                    # Moving Objectì— WARNING ë¶€ì—¬                            
                    # track_id_{track_id}_count ì •ì˜
                    try:
                        globals()[f'track_id_{track_id}_count_{sequence}'] += 1 # ğŸ­
                    except:
                        globals()[f'track_id_{track_id}_count_{sequence}'] = 1 # ğŸ­
                                                                                            
                    
                    # WARNINGì„ ì£¼ëŠ” ê¸°ì¤€ : warning_count
                    # warning ì¡°ê±´ ë‹¬ì„±
                    if globals()[f'track_id_{track_id}_count_{sequence}'] >= warning_count: # ğŸŒˆ ğŸŒ
                        cv2.putText(frame, "WARNING", 
                                    (int(x1), int(y1) - 30), 
                                    cv2.FONT_HERSHEY_SIMPLEX, 
                                    #2.5, (0, 0, 255), thickness=3) # ğŸŒˆ
                                    2.5, (0, 165, 255), thickness=3) # ğŸŒˆ
                        
                        print(f'Watch out for {class_name}!!!')

                        # ë§Œì•½ í•˜ë‚˜ì˜ track_id ì—ì„œ WARNINGì´ ë°œìƒí•œë‹¤ë©´ ë‚˜ë¨¸ì§€ëŠ” 0ìœ¼ë¡œ ì´ˆê¸°í™”
                        for i in track_ids:  # ğŸ­
                            if track_id == i: # ğŸŒˆ
                                pass # ğŸŒˆ
                            else: # ğŸŒˆ
                                #globals()[f'track_id_{track_id}_count_{sequence}'] = 0  # ğŸŒˆ
                                globals()[f'track_id_{track_id}_count_{sequence}'] = globals()[f'track_id_{track_id}_count_{sequence}'] // 2 # â­•ï¸


                        # LEDì— ì‹ í˜¸ë¥¼ ì£¼ëŠ” ë³€ìˆ˜ # ğŸ£    
                        if (class_name == 'car') | (class_name == 'motorbike') | (class_name == 'truck'): # ğŸ£
                            warning_text = 'car' # ğŸ£
                        elif (class_name == 'person') | (class_name == 'bicycle'): # ğŸ£
                            warning_text = 'person' # ğŸ£
                                                    
                    # warning ì¡°ê±´ ë¯¸ë‹¬ì„±
                    else: 
                        pass
                        
                else: 
                    cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (colors[track_id % len(colors)]), 3)
                    cv2.putText(frame, f"Stopping{track_id}_{class_name}",                                         
                                (int(x1), int(y1) - 10), 
                                cv2.FONT_HERSHEY_SIMPLEX, 0.9, 
                                colors[track_id % len(colors)], 
                                thickness=2)
            
    return [tracks_info, warning_text]


def YOLO_OB_OT_moving(data_path, vidoe_name1, vidoe_name2, 
                      detection_threshold = 0.5, 
                      detect_type = 'None', 
                      weight = None, 
                      ud_square = 1, 
                      warning_count = 10, 
                      alraming_time = 500):

    
    cap1 = load_video(data_path, vidoe_name1) # ğŸŒ
    cap2 =  load_video(data_path, vidoe_name2) # ğŸŒ
    
    ret1, frame1 = cap1.read() # ğŸŒ
    ret2, frame2 = cap2.read() # ğŸŒ

    video_out_path = os.path.join(data_path, 'data', 'output', f'{vidoe_name1}_th{detection_threshold}_{detect_type}_hp{weight}_us{ud_square}_wc{warning_count}_at{alraming_time}.mp4') # ğŸŒ
    
    final_frame_shape = (1500, 500) # ğŸ­
    cap_out = cv2.VideoWriter(video_out_path, 
                              cv2.VideoWriter_fourcc(*'MP4V'), 
                              cap1.get(cv2.CAP_PROP_FPS), # ğŸŒ cap1ì˜ í”„ë ˆì„ì†ë„
                              (final_frame_shape[0], final_frame_shape[1])) # ğŸ­
    
    # define model
    model = YOLO("yolov8n.pt")
    tracker1 = Tracker() # ğŸŒ
    tracker2 = Tracker() # ğŸŒ
    
    # etc
    colors = [(random.randint(0, 255), random.randint(0, 255), random.randint(0, 255)) for j in range(10)]
    cl_name = pd.read_csv(os.path.join(data_path, 'coco_classes.txt'))
    class_names = cl_name['class_name'].tolist()

    # live monitoring
    cv2.namedWindow("Tracking", cv2.WINDOW_NORMAL) # GPT ì¶”ê°€ : ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
    cv2.resizeWindow("Tracking", final_frame_shape[0], final_frame_shape[1]) # GPT ì¶”ê°€ : ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§

    monitor_width = 1500
    monitor_height = 500

    before_track_infos1 = []
    before_track_infos2 = []

    # LED : WANRING ì‹ í˜¸ë¥¼ ì£¼ëŠ” ê¸°ì¤€
    warning_text = ''  # ğŸ­
    warning_text1 = ''
    warning_text2 = '' 
    warning_count_down = 0
    c = 0

    #  
    target_classes = ['car', 'person', 'motorbike', 'bicycle', 'truck'] # ğŸ£

    while ret1 and ret2:

        for result1, result2 in zip(model(frame1, device="mps"), model(frame2, device="mps")):
            detections1 = []
            detections2 = []

            detections1 = yolov8(result1, detection_threshold, class_names, target_classes)
            detections2 = yolov8(result2, detection_threshold, class_names, target_classes)

            tracker1.update(frame1, detections1)
            first_track1_dict = first_track(tracker1, 1, class_names, before_track_infos1 = before_track_infos1, before_track_infos2 = before_track_infos2, target_classes = target_classes)
            second_track1 = second_track(frame1, tracker1, 1, detect_type, first_track1_dict, warning_text=warning_text, warning_count_down=warning_count_down, before_track_infos1 = before_track_infos1, before_track_infos2 = before_track_infos2)
            
            tracker2.update(frame2, detections2)
            first_track2_dict = first_track(tracker2, 2, class_names, before_track_infos1 = before_track_infos1, before_track_infos2 = before_track_infos2, target_classes = target_classes)
            second_track2 = second_track(frame2, tracker2, 2, detect_type, first_track2_dict, warning_text=warning_text, warning_count_down=warning_count_down, before_track_infos1 = before_track_infos1, before_track_infos2 = before_track_infos2)
                        
            before_track_infos1 = second_track1[0].copy()
            before_track_infos2 = second_track2[0].copy()
            


        # Resize frame # ğŸŒˆ
        dvided_width = monitor_width // 3
        frame1_resized = cv2.resize(frame1, (dvided_width, monitor_height)) # ğŸŒ
        frame2_resized = cv2.resize(frame2, (dvided_width, monitor_height)) # ğŸŒ

        # Create final frame # ğŸŒˆ
        final_frame = np.zeros((monitor_height, monitor_width, 3), dtype=np.uint8) # ğŸ£
        
        # 1) left 
        final_frame[:, :dvided_width] = frame1_resized # ğŸ£
        # 2) middle 
        final_frame[:, dvided_width:dvided_width * 2] = np.zeros((monitor_height, dvided_width, 3), dtype=np.uint8) # ğŸ£
        
        # 3) right
        final_frame[:, dvided_width * 2:] = frame2_resized # ğŸŒ



        # ì–‘ìª½ ëª¨ë‘ warning ë°œìƒ ì‹œ second_track1[2] ì—…ë°ì´íŠ¸ ë¨
        if (second_track1[1] != '') & (second_track2[1] != ''): # ë‘ ì˜ìƒì— ëª¨ë‘ WARNING ì‹ í˜¸ë¥¼ ì¤„ ë•Œ # â­•ï¸
            c += 1
            warning_text1 = second_track1[1]
            warning_text2 = second_track2[1]
            

            try:  # â­•ï¸
                if cond1: # ì•ŒëŒì´ ìš¸ë¦¬ëŠ” ì¤‘ : warning_count_down ì—…ë°ì´íŠ¸ X
                    pass
                else: # ì•ŒëŒì´ ë§ˆì¹˜ë©´ : warning_count_down ì—…ë°ì´íŠ¸ O
                    warning_count_down = alraming_time # ğŸ’Œ
            except:
                warning_count_down = alraming_time # ğŸ’Œ
        
        if c == 1: # â­•ï¸
            real_warning_text1 = warning_text1
            real_warning_text2 = warning_text2

        # cond : 
        cond1 = ((alraming_time * 2 // 4) < warning_count_down) & (warning_count_down <= alraming_time) # â­•ï¸
        # ë³µì¡í•œ ìƒí™©ì—ì„œëŠ” ë„ˆë¬´ ê°™ì€ ì‹ í˜¸ë¥¼ ë„ˆë¬´ ì˜¤ë˜ ì œê³µ : alraming_time * 2 // 5) < warning_count_down
        
        if cond1 : # â­•ï¸
            real_warning_text1 = real_warning_text1
            real_warning_text2 = real_warning_text2
            warning_count_down -= 1
        else: # â­•ï¸
            real_warning_text1 = warning_text1
            real_warning_text2 = warning_text2
    
        n = 6
        cond2_on = warning_count_down % (alraming_time // n) >= ((alraming_time // n) // 2) # â­•ï¸
        
        if cond2_on: # â­•ï¸
            start_warning_alram = 1 
        else: 
            start_warning_alram = 0 



        car_path = './car.png'
        person_path = './person.png'
        warn_path = './warn.png'
        crash_path = './crash.png'

        car_img = cv2.imread(car_path)
        car_img = cv2.resize(car_img, (200, 300))

        person_img = cv2.imread(person_path)
        person_img = cv2.resize(person_img, (200, 300))

        crash_img = cv2.imread(crash_path)
        crash_img = cv2.resize(crash_img, (100, 200))

        warn_img = cv2.imread(warn_path)
        warn_img = cv2.resize(warn_img, (500, 200))

        
        if (cond1) & (start_warning_alram  == 1): # ğŸ­ # ğŸ£  # â­•ï¸

            # left
            if real_warning_text1 == 'car':
                final_frame[:300, 500:700] = car_img
            elif real_warning_text1 == 'person':
                final_frame[:300, 500:700] = person_img
                
            # right
            if real_warning_text2 == 'car':                
                final_frame[:300, 800:1000] = car_img
            elif real_warning_text2 == 'person':
                final_frame[:300, 800:1000] = person_img
            
            final_frame[50:250, 700:800] = crash_img
            final_frame[300:500, 500:1000] = warn_img
            

        # live monitoring
        cv2.imshow("Tracking", final_frame) # ğŸŒˆ
        cap_out.write(final_frame) # ğŸ­
        if cv2.waitKey(1) == ord('q'): 
            break

        ret1, frame1 = cap1.read()
        ret2, frame2 = cap2.read()

    cap1.release()
    cap2.release()
    cap_out.release()
    cv2.destroyAllWindows()


#for video in vidoe_name_list:
YOLO_OB_OT_moving(data_path=data_path,
                vidoe_name1=video1, 
                vidoe_name2=video2, 
                detection_threshold = detection_threshold, 
                detect_type=detect_type,
                weight=weight,
                ud_square = ud_square,
                warning_count = warning_count,
                alraming_time = alraming_time)